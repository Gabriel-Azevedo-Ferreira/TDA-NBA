---
title: "R Notebook"
output: html_notebook
---
First I started with data non normalized by minutes played (the one who gave better results in PCA)
```{r}
library(igraph)
library(TDAmapper)
library(fastcluster)
library(FactoMineR)
library(ggplot2)

getwd()

#import data
df <- read.csv("../../Data/NBA-data-With-Positions.csv")

# labels
label <- df$PosicAbbrev
#levels(label) <- c(5,4,1,3,2)

# set index
rownames(df) <- df[,'Player']

# features selection -> new smaller dataset -> distance matrix
chosen_features = c('REB','AST','TOV','STL','BLK','PF','PTS')
X <- as.matrix(df[chosen_features])

normX <- t(t(X)/sqrt(diag(var(X)))) # dividing columns by the variance
                                    # (normalizing without centering)
d <- dist(normX,method = "euclidian") # This way I get what Alagapin called
                                      # "Variance normalized euclidean distance"

#Taking filter function from PCA
myPCA <- PCA(X,graph = F)
plot.PCA(myPCA,axes = c(1,2),choix = "var")
plot.PCA(myPCA,axes = c(1,2),choix = "ind", col.ind = label, label = "ind.sup")

filter <- list(matrix(myPCA$ind$coord[,1]),matrix(myPCA$ind$coord[,2]))

?mapper1D

#cluster_cutoff_at_first_empty_bin(1,1,1)

map    <- mapper2D(
          distance_matrix = d, 
          filter_values = filter,
          num_intervals = c(30,30),#20,20 or 30,30
          percent_overlap = 50,
          num_bins_when_clustering = 5 # TODO understand and tune. Has to do with 
                                       # the cut the hierarchical clustering tree
      )

#plot
g <- graph.adjacency(map$adjacency, mode="undirected")
#TODO
# learn how to color the graph and decide a way to do it
plot(g, layout = layout.auto(g))

```


Same thing for normilized data (divided per minutes played)
```{r}
#import data
df <- read.csv("../../Data/NBA_per_minute_complete_with_positions.csv")

# labels
label <- df$PosicAbbrev
#levels(label) <- c(5,4,1,3,2)

# features selection -> new smaller dataset -> distance matrix
chosen_features = c('REB','AST','TOV','STL','BLK','PF','PTS')
X <- as.matrix(df[chosen_features])

normX <- t(t(X)/sqrt(diag(var(X)))) # dividing columns by the variance
                                    # (normalizing without centering)
d <- dist(normX,method = "euclidian") # This way I get what Alagapin called
                                      # "Variance normalized euclidean distance"

#Taking filter function from PCA
myPCA <- PCA(X,graph = F)
plot.PCA(myPCA,axes = c(1,2),choix = "var")
plot.PCA(myPCA,axes = c(1,2),choix = "ind", col.ind = label, label = "ind.sup")

filter <- list(matrix(myPCA$ind$coord[,1]),matrix(myPCA$ind$coord[,2]))

?mapper1D

#cluster_cutoff_at_first_empty_bin(1,1,1)

map    <- mapper2D(
          distance_matrix = d, 
          filter_values = filter,
          num_intervals = c(30,30),#20,20 or 30,30
          percent_overlap = 50,
          num_bins_when_clustering = 5 # TODO understand and tune. Has to do with 
                                       # the cut the hierarchical clustering tree
      )

#plot
g <- graph.adjacency(map$adjacency, mode="undirected")
#TODO
# learn how to color the graph and decide a way to do it
plot(g, layout = layout.auto(g))
```

